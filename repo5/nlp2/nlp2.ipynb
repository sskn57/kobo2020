{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2章 自然言語処理と単語の分類表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語単位で考える\n",
    "「単語の意味」を捉える\n",
    "シソーラス\n",
    "カウントベース\n",
    "推論ベースの手法(Word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "シソーラス\n",
    "単語を意味ごとにグループ分け\n",
    "意味上の上下関係を構築\n",
    "このように集合としてとらえる\n",
    "\n",
    "人為的なので多くの欠点が存在"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カウントベース\n",
    "コーパス(corpus)という大量のテキストデータを利用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You say goodbye and I say hello.\n",
      "you say goodbye and i say hello.\n",
      "you say goodbye and i say hello .\n",
      "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "print(text)\n",
    "\n",
    "text = text.lower()\n",
    "print(text)\n",
    "\n",
    "text = text.replace('.', ' .')\n",
    "print(text)\n",
    "\n",
    "words = text.split(' ')\n",
    "print(words)\n",
    "\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "for word in words:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word\n",
    "\n",
    "print(word_to_id)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "corpus = [word_to_id[w] for w in words]\n",
    "corpus = np.array(corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーパスの作成ができた。以上の操作を関数preprocess()として作成した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "    \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 1, 5, 6]),\n",
       " {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6},\n",
       " {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#実行例\n",
    "text = 'You say goodbye and I say hello.'\n",
    "preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語をRGBのように表現する。すなわちベクトルで表すことを目標にする。これを単語の分散表現という。\\\n",
    "分布仮説; 単語の意味は周囲の単語によって決まるという考え\\\n",
    "コンテキスト; 注目する単語の周囲の単語のこと。\\\n",
    "ウィンドウサイズ; 注目する単語から左右に何単語分コンテキストに含めるかを表す数\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "print(corpus)\n",
    "print(word_to_id)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "共起行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([\n",
    "    [0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 1, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0]\n",
    "], dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(C[0]) #単語ID0のベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(C[6]) #単語ID6のベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(C[word_to_id['goodbye']]) #goodbeyのベクトル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーパスから共起行列を作成する関数create_co_matrix()\\\n",
    "corpusは単語IDのリスト, vocab_sizeは語彙数, window_sizeはウィンドウサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size = 1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype = np.int32) #行列の初期化\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus): #インデックス, 要素\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - 1\n",
    "            right_idx = idx + 1\n",
    "            \n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#実行\n",
    "co_matrix = create_co_matrix(corpus, 7)\n",
    "print(co_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ベクトル間の類似度\\\n",
    "コサイン類似度; 2つのベクトルについて、分子に内積、分母に各ベクトルのノルム(大きさ)で定められる。これはcosに相当し、向きの類似度を表す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_simlarity(x, y, eps = 1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x**2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y**2)) + eps)\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067691154799\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#実行\n",
    "text = \"You say goodbye and I say  hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(corpus)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "c0 = C[word_to_id['you']]\n",
    "c1 = C[word_to_id['i']]\n",
    "c2 = C[word_to_id['say']]\n",
    "\n",
    "print(cos_simlarity(c0, c1))\n",
    "print(cos_simlarity(c0, c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.3535533863255068\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#実行\n",
    "text = \"You say goodbye and I say hello. We are good friends. I talk with you freely\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(corpus)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "c0 = C[word_to_id['talk']]\n",
    "c1 = C[word_to_id['are']]\n",
    "c2 = C[word_to_id['say']]\n",
    "\n",
    "print(cos_simlarity(c0, c1))\n",
    "print(cos_simlarity(c0, c2))\n",
    "print(cos_simlarity(c1, c2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most_similar()関数の実装\n",
    "ある単語がクエリとして与えられたとき、そのクエリに対して類似した単語をランキング形式で表示する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top = 5):\n",
    "    #クエリを取り出す\n",
    "    if query not in word_to_id:\n",
    "        print('%s is not found' % query)\n",
    "        return\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "    \n",
    "    #コサイン類似度の算出\n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_simlarity(word_matrix[i], query_vec)\n",
    "        \n",
    "    #コサイン類似度が高い順に出力\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort(): #argsort()はインデックスを小さい順に並べる -1を掛けると大きい順となる\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "        \n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " goodbye: 0.7071067691154799\n",
      " i: 0.7071067691154799\n",
      " hello: 0.7071067691154799\n",
      " say: 0.0\n",
      " and: 0.0\n"
     ]
    }
   ],
   "source": [
    "#実行\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "most_similar('you', word_to_id, id_to_word, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "簡単ではあるが、カウントベース手法を実装できた。主語である'i'が高くなっていることが確認できたが、'goodbye', 'hello'といった'you'とはあまり類似していないような単語も同様の数値を示している。これはtextが少ないからであると考えられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " i: 0.8017837198720819\n",
      " hello: 0.8017837170373484\n",
      " house: 0.6172133949957571\n",
      " goodbye: 0.46291004624681786\n",
      " go: 0.30860669749787856\n"
     ]
    }
   ],
   "source": [
    "#実行\n",
    "text = \"You say goodbye and I say hello. You go to my house. I go to your house. I do not say goodbye to you.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "most_similar('you', word_to_id, id_to_word, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " PMI(相互情報量)の実装\\\n",
    "$$PMI(x,y) = log_2\\frac{P(x,y)}{P(x)P(y)}$$\n",
    "$$PMI(x,y) = log_2\\frac{\\frac{P(x,y)}{N}}{\\frac{P(x)}{N}\\frac{P(y)}{N}} = log_2\\frac{C(x,y)N}{C(x)C(y)}$$\n",
    "$P(x)$, $P(y)$は単語x, yの出現する確率\\\n",
    "$P(x,y)$は単語x, yの共起する確率\\\n",
    "$C(x,y)$は単語x, yの共起する回数\\\n",
    "$C(x)$, $C(y)$は単語x, yの出現回数\\\n",
    "$N$はコーパスに含まれる単語数\\\n",
    "$C(x, y)$が0のとき$log_20=-\\infty$となるため、正の相互情報量PPMIを用いる。\n",
    "$$PPMI(x, y) = max(0, PMI(x,y))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmi(C, verbose = False, eps = 1e-8):\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis = 0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(C.shape[0]): #行\n",
    "        for j in range(C.shape[1]): #列\n",
    "            pmi = np.log2(C[i, j] * N / (S[i] * S[j]) + eps)\n",
    "            M[i,j] = max(0, pmi)\n",
    "            \n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total/100) == 0:\n",
    "                    print('%.1f%% done' % (100*cnt/total))\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance matrix\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "------------------------------------------------------------\n",
      "PPMI\n",
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "#実行\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "W = ppmi(C)\n",
    "\n",
    "np.set_printoptions(precision = 3)\n",
    "print('covariance matrix')\n",
    "print(C)\n",
    "print('-'*60)\n",
    "print('PPMI')\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      "[0.    0.807 0.    1.807 0.    0.    0.   ]\n"
     ]
    }
   ],
   "source": [
    "print(W[word_to_id['you']])\n",
    "print(W[word_to_id['goodbye']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "共起行列からPPMI行列に変換できた。\n",
    "重要度の低い要素は0となっている。この手法では単語数がそのまま次元数になるため、大きなコーパスを扱うのにあまり向いていないと考えられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次元削減\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size, window_size=1)\n",
    "W = ppmi(C)\n",
    "\n",
    "#SVD\n",
    "U, S, V = np.linalg.svd(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n",
      "(7, 7)\n",
      "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      "(7, 7)\n",
      "[-3.409e-01 -1.110e-16 -3.886e-16 -1.205e-01  0.000e+00  9.323e-01\n",
      "  2.226e-16]\n",
      "(7, 7)\n",
      "[-3.409e-01 -1.110e-16]\n"
     ]
    }
   ],
   "source": [
    "#確認\n",
    "print(C[0]) #共起行列\n",
    "print(C.shape)\n",
    "print(W[0]) #PPMI行列\n",
    "print(W.shape)\n",
    "print(U[0]) #SVD\n",
    "print(U.shape)\n",
    "print(U[0, :2]) #次元削減"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2次元に次元削減ができている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for word, word_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
    "    #annoatate(String, (x, y)) は(x, y)にStringを描画\n",
    "#print(U[:, 0])\n",
    "plt.scatter(U[:, 0], U[:, 1], alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVDではi, goodbye, youが比較的近い位置に位置していることが分かった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PTBデータセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ptb.train.txt ... \n",
      "Done\n",
      "corpus_size: 929589\n",
      "corpus[:30]: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "id_to_word[0]: aer\n",
      "id_to_word[1]: banknote\n",
      "id_to_word[2]: berlitz\n",
      "id_to_word[len(id_to_word)-1] unilab\n",
      "\n",
      "word_to_id['car']: 3856\n",
      "word_to_id['happy']: 4428\n",
      "wors_to_id['lexus']: 7426\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train') #preprocessと同じ扱い方,trainは訓練用, testはテスト用, validは検証用\n",
    "\n",
    "#確認用\n",
    "print('corpus_size:', len(corpus))\n",
    "print('corpus[:30]:', corpus[:30])\n",
    "print()\n",
    "print('id_to_word[0]:', id_to_word[0])\n",
    "print('id_to_word[1]:', id_to_word[1])\n",
    "print('id_to_word[2]:', id_to_word[2])\n",
    "print('id_to_word[len(id_to_word)-1]', id_to_word[len(id_to_word)-1])\n",
    "print()\n",
    "print(\"word_to_id['car']:\", word_to_id['car'])\n",
    "print(\"word_to_id['happy']:\", word_to_id['happy'])\n",
    "print(\"wors_to_id['lexus']:\", word_to_id['lexus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting co-occurrence ...\n",
      "calsulateing PPMI ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ken19\\myFIles\\my-kobo\\repo5\\nlp2\\common\\util.py:139: RuntimeWarning: overflow encountered in long_scalars\n",
      "  pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
      "C:\\Users\\ken19\\myFIles\\my-kobo\\repo5\\nlp2\\common\\util.py:139: RuntimeWarning: invalid value encountered in log2\n",
      "  pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0% done\n",
      "2.0% done\n",
      "3.0% done\n",
      "4.0% done\n",
      "5.0% done\n",
      "6.0% done\n",
      "7.0% done\n",
      "8.0% done\n",
      "9.0% done\n",
      "10.0% done\n",
      "11.0% done\n",
      "12.0% done\n",
      "13.0% done\n",
      "14.0% done\n",
      "15.0% done\n",
      "16.0% done\n",
      "17.0% done\n",
      "18.0% done\n",
      "19.0% done\n",
      "20.0% done\n",
      "21.0% done\n",
      "22.0% done\n",
      "23.0% done\n",
      "24.0% done\n",
      "25.0% done\n",
      "26.0% done\n",
      "27.0% done\n",
      "28.0% done\n",
      "29.0% done\n",
      "30.0% done\n",
      "31.0% done\n",
      "32.0% done\n",
      "33.0% done\n",
      "34.0% done\n",
      "35.0% done\n",
      "36.0% done\n",
      "37.0% done\n",
      "38.0% done\n",
      "39.0% done\n",
      "40.0% done\n",
      "41.0% done\n",
      "42.0% done\n",
      "43.0% done\n",
      "44.0% done\n",
      "45.0% done\n",
      "46.0% done\n",
      "47.0% done\n",
      "48.0% done\n",
      "49.0% done\n",
      "50.0% done\n",
      "51.0% done\n",
      "52.0% done\n",
      "53.0% done\n",
      "54.0% done\n",
      "55.0% done\n",
      "56.0% done\n",
      "57.0% done\n",
      "58.0% done\n",
      "59.0% done\n",
      "60.0% done\n",
      "61.0% done\n",
      "62.0% done\n",
      "63.0% done\n",
      "64.0% done\n",
      "65.0% done\n",
      "66.0% done\n",
      "67.0% done\n",
      "68.0% done\n",
      "69.0% done\n",
      "70.0% done\n",
      "71.0% done\n",
      "72.0% done\n",
      "73.0% done\n",
      "74.0% done\n",
      "75.0% done\n",
      "76.0% done\n",
      "77.0% done\n",
      "78.0% done\n",
      "79.0% done\n",
      "80.0% done\n",
      "81.0% done\n",
      "82.0% done\n",
      "83.0% done\n",
      "84.0% done\n",
      "85.0% done\n",
      "86.0% done\n",
      "87.0% done\n",
      "88.0% done\n",
      "89.0% done\n",
      "90.0% done\n",
      "91.0% done\n",
      "92.0% done\n",
      "93.0% done\n",
      "94.0% done\n",
      "95.0% done\n",
      "96.0% done\n",
      "97.0% done\n",
      "98.0% done\n",
      "99.0% done\n",
      "calculateing SVD ...\n",
      "\n",
      "[query] you\n",
      " i: 0.7000651359558105\n",
      " we: 0.6590059995651245\n",
      " anybody: 0.5515195727348328\n",
      " do: 0.5454789400100708\n",
      " me: 0.5316669940948486\n",
      "\n",
      "[query] year\n",
      " month: 0.693265438079834\n",
      " earlier: 0.6337158679962158\n",
      " next: 0.6086169481277466\n",
      " quarter: 0.597454309463501\n",
      " last: 0.5956025123596191\n",
      "\n",
      "[query] car\n",
      " luxury: 0.6569687724113464\n",
      " auto: 0.565083384513855\n",
      " cars: 0.5150364637374878\n",
      " lexus: 0.48423635959625244\n",
      " truck: 0.48398295044898987\n",
      "\n",
      "[query] toyota\n",
      " motor: 0.721107006072998\n",
      " nissan: 0.6842175126075745\n",
      " motors: 0.6827226877212524\n",
      " honda: 0.6163697838783264\n",
      " lexus: 0.5953662991523743\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import most_similar, create_co_matrix, ppmi\n",
    "\n",
    "\n",
    "window_sixe = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "print('counting co-occurrence ...')\n",
    "C = create_co_matrix(corpus, vocab_size, window_sixe)\n",
    "\n",
    "print('calsulateing PPMI ...')\n",
    "W = ppmi(C, verbose = True)\n",
    "\n",
    "print('calculateing SVD ...')\n",
    "try:\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5, random_state=None)\n",
    "except ImportError:\n",
    "    U, S, V = np.linalg.avd(W)\n",
    "    \n",
    "word_vecs = U[:, :wordvec_size]\n",
    "\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クエリyouに対してi, someone, we, anybody, somebodyなどの似たような働きを示す単語が上位に来ている。また、最も近いであろうiが類似度約0,7であり、weは0.66程度である。上位3位以降が0.55程度であることを考えると、iやweが他の単語よりyouに類似していると言える。\\\n",
    "クエリyearに対してはmonth, last, quarter, earlier, juneなどの副詞に相当する単語が上位を占めている。中でも、年に意味的に近いmonthが類似度0.69程度であり、だいぶ類似していると言える。\\\n",
    "このように感覚的に近いと思える単語が上位に来ていることが確認できた。\\\n",
    "クエリcarの場合は同じ意味のautoが類似度約0.66で最も上位になっている。複数形carsよりも類似度が高い結果なっている。また、一見類似していないように思えるluxuryが上位に来ている。このように感覚から外れている単語が表れることもあることが分かった。\\\n",
    "クエリtoyotaの場合は、motor, motorsが上位に来ている。これはtoyotaとmotorの単語の結びつきが非常に強いことを示している。類似度も約0.72と非常に大きいことが分かる。また、上記3つの単語に比べ全体的に類似度が高いことも判断できる。motor(s)以外には競合の企業を表す単語が上位に来ている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] encourage\n",
      " educate: 0.602806806564331\n",
      " advise: 0.5481181144714355\n",
      " urge: 0.5113472938537598\n",
      " regulate: 0.5081126689910889\n",
      " attract: 0.4796712398529053\n",
      "\n",
      "[query] moment\n",
      " minute: 0.47832849621772766\n",
      " phenomenon: 0.4470707178115845\n",
      " mid-1970s: 0.43929964303970337\n",
      " reason: 0.42851513624191284\n",
      " obstacle: 0.4188341498374939\n",
      "\n",
      "[query] hope\n",
      " want: 0.41635143756866455\n",
      " comfort: 0.412539005279541\n",
      " ca: 0.39929574728012085\n",
      " ought: 0.35499972105026245\n",
      " pressure: 0.3437035083770752\n",
      "\n",
      "[query] mix\n",
      " posture: 0.5046809315681458\n",
      " specifications: 0.45210132002830505\n",
      " attitude: 0.42671626806259155\n",
      " roots: 0.4209299087524414\n",
      " sweep: 0.4151420593261719\n",
      "\n",
      "[query] word\n",
      " tremor: 0.40928590297698975\n",
      " figuring: 0.4060996174812317\n",
      " passed: 0.3936804533004761\n",
      " setback: 0.38990336656570435\n",
      " aspect: 0.38482341170310974\n",
      "\n",
      "[query] spring\n",
      " february: 0.5507664680480957\n",
      " march: 0.5269663333892822\n",
      " april: 0.5212395787239075\n",
      " month: 0.49606606364250183\n",
      " june: 0.48717236518859863\n"
     ]
    }
   ],
   "source": [
    "querys = ['encourage', 'moment', 'hope', 'mix', 'word', 'spring']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は理解を深めるために、類似度の高い組を見つけることを通して、類似度を高くする条件について考える。\\\n",
    "いくつか思いついた単語をクエリとして試した。全体的に類似度は0.5程度であり、参考文献1の実行例のように0.7程度になあるクエリではなかった。そこで一番類似度が高かったencourage, educateが動詞であることに注目して、一般的な動詞をクエリに選んで試した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] drive\n",
      " disk: 0.5266988277435303\n",
      " wipe: 0.4558466076850891\n",
      " ride: 0.43623366951942444\n",
      " slow: 0.3960356116294861\n",
      " break: 0.3730338215827942\n",
      "\n",
      "[query] stand\n",
      " lay: 0.4818665385246277\n",
      " defend: 0.4704436659812927\n",
      " throw: 0.4553096890449524\n",
      " turn: 0.4479348361492157\n",
      " rulings: 0.44661208987236023\n",
      "\n",
      "[query] move\n",
      " changes: 0.41549545526504517\n",
      " step: 0.3945024609565735\n",
      " pill: 0.3662164807319641\n",
      " translate: 0.3565753996372223\n",
      " accord: 0.35581302642822266\n",
      "\n",
      "[query] touch\n",
      " preclude: 0.45137763023376465\n",
      " tightly: 0.44789525866508484\n",
      " ring: 0.431865930557251\n",
      " recommend: 0.4127454161643982\n",
      " break: 0.4089815020561218\n",
      "\n",
      "[query] go\n",
      " stay: 0.5369476675987244\n",
      " walk: 0.5275244116783142\n",
      " lay: 0.46299460530281067\n",
      " went: 0.46055522561073303\n",
      " wipe: 0.4540635943412781\n",
      "\n",
      "[query] feel\n",
      " know: 0.650989830493927\n",
      " think: 0.610784649848938\n",
      " guess: 0.5612886548042297\n",
      " sorry: 0.5461536049842834\n",
      " sure: 0.5056599378585815\n",
      "\n",
      "[query] say\n",
      " believe: 0.6032611131668091\n",
      " agree: 0.5431695580482483\n",
      " contend: 0.5334901809692383\n",
      " expect: 0.5262507796287537\n",
      " alike: 0.49212804436683655\n",
      "\n",
      "[query] come\n",
      " go: 0.43246179819107056\n",
      " preclude: 0.4201066493988037\n",
      " bother: 0.41215914487838745\n",
      " fall: 0.41184866428375244\n",
      " pick: 0.41012826561927795\n"
     ]
    }
   ],
   "source": [
    "querys = ['drive', 'stand', 'move', 'touch', 'go', 'feel', 'say', 'come']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果は先ほどよりも類似度が大きい組を得ることができた。\\\n",
    "say, believeの組が0.6程度である。比較的高くなったsayやfeelは言い換えに相当する単語が存在していて、かつその単語も比較的頻繁に使われる単語であるからではないかと考えた。\\\n",
    "そこで「言う」として、talk, speak, tell, call, express, declare, state, assert, remarkを考えた。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] talk\n",
      " tell: 0.5961438417434692\n",
      " radio: 0.4074557423591614\n",
      " live: 0.39927658438682556\n",
      " jokes: 0.3906444311141968\n",
      " else: 0.3864635229110718\n",
      "\n",
      "[query] speak\n",
      " shoot: 0.49664413928985596\n",
      " catch: 0.49352118372917175\n",
      " unstable: 0.48761290311813354\n",
      " modify: 0.4774388372898102\n",
      " serve: 0.46347030997276306\n",
      "\n",
      "[query] tell\n",
      " know: 0.7003185153007507\n",
      " appreciate: 0.6545106172561646\n",
      " talk: 0.5961438417434692\n",
      " see: 0.5744364261627197\n",
      " exactly: 0.5590279698371887\n",
      "\n",
      "[query] call\n",
      " attract: 0.4159988760948181\n",
      " find: 0.3989780843257904\n",
      " send: 0.39178466796875\n",
      " get: 0.38503310084342957\n",
      " consider: 0.37670081853866577\n",
      "\n",
      "[query] express\n",
      " american: 0.46856221556663513\n",
      " telephone: 0.4569395184516907\n",
      " railway: 0.455424427986145\n",
      " bristol-myers: 0.44312238693237305\n",
      " electric: 0.4361533522605896\n",
      "\n",
      "[query] declare\n",
      " liked: 0.44081801176071167\n",
      " rebuild: 0.41295748949050903\n",
      " hints: 0.411799818277359\n",
      " detected: 0.3875722587108612\n",
      " abruptly: 0.3850403130054474\n",
      "\n",
      "[query] state\n",
      " michigan: 0.6478172540664673\n",
      " california: 0.5979580879211426\n",
      " federal: 0.5709452033042908\n",
      " appellate: 0.5614461898803711\n",
      " pennsylvania: 0.5429211258888245\n",
      "assert is not found\n",
      "\n",
      "[query] remark\n",
      " haunts: 0.4886605441570282\n",
      " eddington: 0.4397304058074951\n",
      " boasts: 0.4359341859817505\n",
      " andersson: 0.43250772356987\n",
      " chan: 0.4309157431125641\n"
     ]
    }
   ],
   "source": [
    "querys = ['talk', 'speak', 'tell', 'call', 'express', 'declare', 'state', 'assert', 'remark']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果から判断すると、tell, stateを除き、あまり類似度の高い組は得られていない。類似度の観点で単語の意味だけではなく他の要素も考慮しなくてはいけないことが分かった。\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] of\n",
      " the: 0.6730115413665771\n",
      " a: 0.594205915927887\n",
      " in: 0.5025878548622131\n",
      " <unk>: 0.4969627857208252\n",
      " for: 0.4781818985939026\n",
      "\n",
      "[query] as\n",
      " such: 0.5246642827987671\n",
      " well: 0.4252268671989441\n",
      " voiced: 0.36668288707733154\n",
      " tourist: 0.35225406289100647\n",
      " multiples: 0.34371283650398254\n",
      "\n",
      "[query] in\n",
      " of: 0.5025878548622131\n",
      " the: 0.5000847578048706\n",
      " and: 0.48302239179611206\n",
      " <unk>: 0.47925493121147156\n",
      " from: 0.4732705354690552\n",
      "\n",
      "[query] at\n",
      " economist: 0.5247272253036499\n",
      " underwriter: 0.46384668350219727\n",
      " dean: 0.4603249430656433\n",
      " analyst: 0.44740813970565796\n",
      " strategist: 0.43537503480911255\n",
      "\n",
      "[query] on\n",
      " est: 0.4013757109642029\n",
      " the: 0.3889485001564026\n",
      " of: 0.37513166666030884\n",
      " prosecutorial: 0.3638300597667694\n",
      " trafficking: 0.34396201372146606\n",
      "\n",
      "[query] from\n",
      " in: 0.4732705354690552\n",
      " million: 0.4259234070777893\n",
      " stemmed: 0.42048734426498413\n",
      " <unk>: 0.40884679555892944\n",
      " and: 0.39975014328956604\n",
      "\n",
      "[query] by\n",
      " murdoch: 0.3837844133377075\n",
      " rated: 0.3755989670753479\n",
      " directly: 0.3427659869194031\n",
      " was: 0.3412964344024658\n",
      " federally: 0.33749157190322876\n",
      "\n",
      "[query] with\n",
      " between: 0.3636857867240906\n",
      " signed: 0.3619125783443451\n",
      " agreement: 0.361402690410614\n",
      " oak: 0.3591645359992981\n",
      " unfairly: 0.3503415584564209\n",
      "\n",
      "[query] along\n",
      " beyond: 0.4010334610939026\n",
      " glass: 0.3961271643638611\n",
      " yes: 0.384970486164093\n",
      " left: 0.34398990869522095\n",
      " anywhere: 0.33908236026763916\n",
      "\n",
      "[query] among\n",
      " ddb: 0.3704115152359009\n",
      " many: 0.37000396847724915\n",
      " lumber: 0.3475465178489685\n",
      " environmentalism: 0.3432614207267761\n",
      " suppliers: 0.3392101526260376\n"
     ]
    }
   ],
   "source": [
    "querys = ['of', 'as', 'in', 'at', 'on', 'from', 'by', 'with', 'along', 'among']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、クエリとして前置詞の場合を考えた。\\\n",
    "結果はsuch asやof theのように比較的でセットで使われることの多い単後の組が類似度が高かった。また、前置詞同士の組の類似度が高くなる傾向があることもわかった。\\\n",
    "一般的に多く使われる単語ではPMIの定義から真数部分の分母$C(x)C(y)$が大きくなってしまい、類似度があまり大きくならないということが確認できた。\\\n",
    "したがって類似度を高くするには定義から$C(x)C(y)$が小さく、$C(x,y)$が大きくなる、すなわち結びつきの強い単語が存在することが予想できるあまり一般的でない単語をクエリに選んであげると類似度の高い組が得られると考えられる。参考文献1の実行例だとhondaなどがよい例である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] 1980s\n",
      " 1960s: 0.47352468967437744\n",
      " 1970s: 0.4041314423084259\n",
      " gyrations: 0.392749547958374\n",
      " bpca: 0.3796602487564087\n",
      " corresponding: 0.37337809801101685\n",
      "\n",
      "[query] california\n",
      " florida: 0.611814558506012\n",
      " michigan: 0.6063764691352844\n",
      " arizona: 0.6037331223487854\n",
      " state: 0.5979580879211426\n",
      " northern: 0.5937544703483582\n",
      "\n",
      "[query] nixon\n",
      " reagan: 0.5862619876861572\n",
      " deng: 0.5707921385765076\n",
      " sherman: 0.5473047494888306\n",
      " patel: 0.5429495573043823\n",
      " bush: 0.5321998000144958\n",
      "\n",
      "[query] hahn\n",
      " achenbaum: 0.500206470489502\n",
      " savaiko: 0.47107070684432983\n",
      " hutchinson: 0.46491456031799316\n",
      " conway: 0.46259185671806335\n",
      " rifenburgh: 0.45032182335853577\n",
      "\n",
      "[query] jerry\n",
      " gerald: 0.7485620975494385\n",
      " thomas: 0.7159087061882019\n",
      " dan: 0.714588463306427\n",
      " terry: 0.6966327428817749\n",
      " patrick: 0.6857182383537292\n",
      "\n",
      "[query] fast-food\n",
      " chain: 0.6683458089828491\n",
      " restaurant: 0.6529269218444824\n",
      " chains: 0.6108901500701904\n",
      " store: 0.6037276983261108\n",
      " grocery: 0.5992923974990845\n",
      "\n",
      "[query] pasadena\n",
      " santa: 0.5984304547309875\n",
      " near: 0.5516723394393921\n",
      " texas: 0.518937349319458\n",
      " ana: 0.5167354345321655\n",
      " calif.: 0.5139095783233643\n",
      "\n",
      "[query] debentures\n",
      " subordinated: 0.9162163138389587\n",
      " eurobonds: 0.852735161781311\n",
      " notes: 0.8295348286628723\n",
      " convertible: 0.8272995948791504\n",
      " due: 0.7946599721908569\n",
      "\n",
      "[query] aluminum\n",
      " eli: 0.5539578199386597\n",
      " minerals: 0.5505362749099731\n",
      " flat-rolled: 0.5296983122825623\n",
      " uniroyal: 0.5246545076370239\n",
      " specialty: 0.5216410756111145\n"
     ]
    }
   ],
   "source": [
    "querys = ['1980s', 'california', 'nixon', 'hahn', 'jerry', 'fast-food', 'pasadena', 'debentures', 'aluminum']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ptb.train.txt内から上述したような特徴を持つと考えられる単語を適当に選び、実行した。\\\n",
    "結果は確かに類似度が高い組が得られた。特にjerryやdebenturesは上位の単語の類似度が非常に高く、上述した特徴を強く持っていると考えられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "類似度を高くするための条件について考えることを通して、よりカウントベースの手法についての理解が深まった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文献\n",
    "1. ゼロから作るDeep Learning --自然言語処理偏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
