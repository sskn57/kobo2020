{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2章 自然言語処理と単語の分類表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語単位で考える\n",
    "「単語の意味」を捉える\n",
    "シソーラス\n",
    "カウントベース\n",
    "推論ベースの手法(Word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "シソーラス\n",
    "単語を意味ごとにグループ分け\n",
    "意味上の上下関係を構築\n",
    "このように集合としてとらえる\n",
    "\n",
    "人為的なので多くの欠点が存在"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カウントベース\n",
    "コーパス(corpus)という大量のテキストデータを利用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You say goodbye and I say hello.\n",
      "you say goodbye and i say hello.\n",
      "you say goodbye and i say hello .\n",
      "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "print(text)\n",
    "\n",
    "text = text.lower()\n",
    "print(text)\n",
    "\n",
    "text = text.replace('.', ' .')\n",
    "print(text)\n",
    "\n",
    "words = text.split(' ')\n",
    "print(words)\n",
    "\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "for word in words:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word\n",
    "\n",
    "print(word_to_id)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "corpus = [word_to_id[w] for w in words]\n",
    "corpus = np.array(corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーパスの作成ができた。以上の操作を関数preprocess()として作成した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "    \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 1, 5, 6]),\n",
       " {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6},\n",
       " {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#実行例\n",
    "text = 'You say goodbye and I say hello.'\n",
    "preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語をRGBのように表現する。すなわちベクトルで表すことを目標にする。これを単語の分散表現という。\\\n",
    "分布仮説; 単語の意味は周囲の単語によって決まるという考え\\\n",
    "コンテキスト; 注目する単語の周囲の単語のこと。\\\n",
    "ウィンドウサイズ; 注目する単語から左右に何単語分コンテキストに含めるかを表す数\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "print(corpus)\n",
    "print(word_to_id)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "共起行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([\n",
    "    [0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 1, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0]\n",
    "], dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(C[0]) #単語ID0のベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(C[6]) #単語ID6のベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(C[word_to_id['goodbye']]) #goodbeyのベクトル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーパスから共起行列を作成する関数create_co_matrix()\\\n",
    "corpusは単語IDのリスト, vocab_sizeは語彙数, window_sizeはウィンドウサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size = 1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype = np.int32) #行列の初期化\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus): #インデックス, 要素\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - 1\n",
    "            right_idx = idx + 1\n",
    "            \n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#実行\n",
    "co_matrix = create_co_matrix(corpus, 7)\n",
    "print(co_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ベクトル間の類似度\\\n",
    "コサイン類似度; 2つのベクトルについて、分子に内積、分母に各ベクトルのノルム(大きさ)で定められる。これはcosに相当し、向きの類似度を表す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_simlarity(x, y, eps = 1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x**2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y**2)) + eps)\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067691154799\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#実行\n",
    "text = \"You say goodbye and I say  hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(corpus)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "c0 = C[word_to_id['you']]\n",
    "c1 = C[word_to_id['i']]\n",
    "c2 = C[word_to_id['say']]\n",
    "\n",
    "print(cos_simlarity(c0, c1))\n",
    "print(cos_simlarity(c0, c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.3535533863255068\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#実行\n",
    "text = \"You say goodbye and I say hello. We are good friends. I talk with you freely\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(corpus)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "c0 = C[word_to_id['talk']]\n",
    "c1 = C[word_to_id['are']]\n",
    "c2 = C[word_to_id['say']]\n",
    "\n",
    "print(cos_simlarity(c0, c1))\n",
    "print(cos_simlarity(c0, c2))\n",
    "print(cos_simlarity(c1, c2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most_similar()関数の実装\n",
    "ある単語がクエリとして与えられたとき、そのクエリに対して類似した単語をランキング形式で表示する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top = 5):\n",
    "    #クエリを取り出す\n",
    "    if query not in word_to_id:\n",
    "        print('%s is not found' % query)\n",
    "        return\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "    \n",
    "    #コサイン類似度の算出\n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_simlarity(word_matrix[i], query_vec)\n",
    "        \n",
    "    #コサイン類似度が高い順に出力\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort(): #argsort()はインデックスを小さい順に並べる -1を掛けると大きい順となる\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "        \n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " goodbye: 0.7071067691154799\n",
      " i: 0.7071067691154799\n",
      " hello: 0.7071067691154799\n",
      " say: 0.0\n",
      " and: 0.0\n"
     ]
    }
   ],
   "source": [
    "#実行\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "most_similar('you', word_to_id, id_to_word, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "簡単ではあるが、カウントベース手法を実装できた。主語である'i'が高くなっていることが確認できたが、'goodbye', 'hello'といった'you'とはあまり類似していないような単語も同様の数値を示している。これはtextが少ないからであると考えられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " i: 0.8017837198720819\n",
      " hello: 0.8017837170373484\n",
      " house: 0.6172133949957571\n",
      " goodbye: 0.46291004624681786\n",
      " go: 0.30860669749787856\n"
     ]
    }
   ],
   "source": [
    "#実行\n",
    "text = \"You say goodbye and I say hello. You go to my house. I go to your house. I do not say goodbye to you.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "most_similar('you', word_to_id, id_to_word, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " PMI(相互情報量)の実装\\\n",
    "$$PMI(x,y) = log_2\\frac{P(x,y)}{P(x)P(y)}$$\n",
    "$$PMI(x,y) = log_2\\frac{\\frac{P(x,y)}{N}}{\\frac{P(x)}{N}\\frac{P(y)}{N}} = log_2\\frac{C(x,y)N}{C(x)C(y)}$$\n",
    "$P(x)$, $P(y)$は単語x, yの出現する確率\\\n",
    "$P(x,y)$は単語x, yの共起する確率\\\n",
    "$C(x,y)$は単語x, yの共起する回数\\\n",
    "$C(x)$, $C(y)$は単語x, yの出現回数\\\n",
    "$N$はコーパスに含まれる単語数\\\n",
    "$C(x, y)$が0のとき$log_20=-\\infty$となるため、正の相互情報量PPMIを用いる。\n",
    "$$PPMI(x, y) = max(0, PMI(x,y))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmi(C, verbose = False, eps = 1e-8):\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis = 0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(C.shape[0]): #行\n",
    "        for j in range(C.shape[1]): #列\n",
    "            pmi = np.log2(C[i, j] * N / (S[i] * S[j]) + eps)\n",
    "            M[i,j] = max(0, pmi)\n",
    "            \n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total/100) == 0:\n",
    "                    print('%.1f%% done' % (100*cnt/total))\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance matrix\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "------------------------------------------------------------\n",
      "PPMI\n",
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "#実行\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "W = ppmi(C)\n",
    "\n",
    "np.set_printoptions(precision = 3)\n",
    "print('covariance matrix')\n",
    "print(C)\n",
    "print('-'*60)\n",
    "print('PPMI')\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      "[0.    0.807 0.    1.807 0.    0.    0.   ]\n"
     ]
    }
   ],
   "source": [
    "print(W[word_to_id['you']])\n",
    "print(W[word_to_id['goodbye']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "共起行列からPPMI行列に変換できた。\n",
    "重要度の低い要素は0となっている。この手法では単語数がそのまま次元数になるため、大きなコーパスを扱うのにあまり向いていないと考えられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次元削減\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size, window_size=1)\n",
    "W = ppmi(C)\n",
    "\n",
    "#SVD\n",
    "U, S, V = np.linalg.svd(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n",
      "(7, 7)\n",
      "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      "(7, 7)\n",
      "[-3.409e-01 -1.110e-16 -3.886e-16 -1.205e-01  0.000e+00  9.323e-01\n",
      "  2.226e-16]\n",
      "(7, 7)\n",
      "[-3.409e-01 -1.110e-16]\n"
     ]
    }
   ],
   "source": [
    "#確認\n",
    "print(C[0]) #共起行列\n",
    "print(C.shape)\n",
    "print(W[0]) #PPMI行列\n",
    "print(W.shape)\n",
    "print(U[0]) #SVD\n",
    "print(U.shape)\n",
    "print(U[0, :2]) #次元削減"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2次元に次元削減ができている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for word, word_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
    "    #annoatate(String, (x, y)) は(x, y)にStringを描画\n",
    "#print(U[:, 0])\n",
    "plt.scatter(U[:, 0], U[:, 1], alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVDではi, goodbye, youが比較的近い位置に位置していることが分かった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PTBデータセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_size: 929589\n",
      "corpus[:30]: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "id_to_word[0]: aer\n",
      "id_to_word[1]: banknote\n",
      "id_to_word[2]: berlitz\n",
      "id_to_word[len(id_to_word)-1] unilab\n",
      "\n",
      "word_to_id['car']: 3856\n",
      "word_to_id['happy']: 4428\n",
      "wors_to_id['lexus']: 7426\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..\\..')\n",
    "from dataset import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train') #preprocessと同じ扱い方,trainは訓練用, testはテスト用, validは検証用\n",
    "\n",
    "#確認用\n",
    "print('corpus_size:', len(corpus))\n",
    "print('corpus[:30]:', corpus[:30])\n",
    "print()\n",
    "print('id_to_word[0]:', id_to_word[0])\n",
    "print('id_to_word[1]:', id_to_word[1])\n",
    "print('id_to_word[2]:', id_to_word[2])\n",
    "print('id_to_word[len(id_to_word)-1]', id_to_word[len(id_to_word)-1])\n",
    "print()\n",
    "print(\"word_to_id['car']:\", word_to_id['car'])\n",
    "print(\"word_to_id['happy']:\", word_to_id['happy'])\n",
    "print(\"wors_to_id['lexus']:\", word_to_id['lexus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting co-occurrence ...\n",
      "calsulateing PPMI ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\common\\util.py:139: RuntimeWarning: overflow encountered in long_scalars\n",
      "  pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
      "..\\common\\util.py:139: RuntimeWarning: invalid value encountered in log2\n",
      "  pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0% done\n",
      "2.0% done\n",
      "3.0% done\n",
      "4.0% done\n",
      "5.0% done\n",
      "6.0% done\n",
      "7.0% done\n",
      "8.0% done\n",
      "9.0% done\n",
      "10.0% done\n",
      "11.0% done\n",
      "12.0% done\n",
      "13.0% done\n",
      "14.0% done\n",
      "15.0% done\n",
      "16.0% done\n",
      "17.0% done\n",
      "18.0% done\n",
      "19.0% done\n",
      "20.0% done\n",
      "21.0% done\n",
      "22.0% done\n",
      "23.0% done\n",
      "24.0% done\n",
      "25.0% done\n",
      "26.0% done\n",
      "27.0% done\n",
      "28.0% done\n",
      "29.0% done\n",
      "30.0% done\n",
      "31.0% done\n",
      "32.0% done\n",
      "33.0% done\n",
      "34.0% done\n",
      "35.0% done\n",
      "36.0% done\n",
      "37.0% done\n",
      "38.0% done\n",
      "39.0% done\n",
      "40.0% done\n",
      "41.0% done\n",
      "42.0% done\n",
      "43.0% done\n",
      "44.0% done\n",
      "45.0% done\n",
      "46.0% done\n",
      "47.0% done\n",
      "48.0% done\n",
      "49.0% done\n",
      "50.0% done\n",
      "51.0% done\n",
      "52.0% done\n",
      "53.0% done\n",
      "54.0% done\n",
      "55.0% done\n",
      "56.0% done\n",
      "57.0% done\n",
      "58.0% done\n",
      "59.0% done\n",
      "60.0% done\n",
      "61.0% done\n",
      "62.0% done\n",
      "63.0% done\n",
      "64.0% done\n",
      "65.0% done\n",
      "66.0% done\n",
      "67.0% done\n",
      "68.0% done\n",
      "69.0% done\n",
      "70.0% done\n",
      "71.0% done\n",
      "72.0% done\n",
      "73.0% done\n",
      "74.0% done\n",
      "75.0% done\n",
      "76.0% done\n",
      "77.0% done\n",
      "78.0% done\n",
      "79.0% done\n",
      "80.0% done\n",
      "81.0% done\n",
      "82.0% done\n",
      "83.0% done\n",
      "84.0% done\n",
      "85.0% done\n",
      "86.0% done\n",
      "87.0% done\n",
      "88.0% done\n",
      "89.0% done\n",
      "90.0% done\n",
      "91.0% done\n",
      "92.0% done\n",
      "93.0% done\n",
      "94.0% done\n",
      "95.0% done\n",
      "96.0% done\n",
      "97.0% done\n",
      "98.0% done\n",
      "99.0% done\n",
      "calculateing SVD ...\n",
      "\n",
      "[query] you\n",
      " i: 0.6145836710929871\n",
      " someone: 0.5487632751464844\n",
      " we: 0.5397644639015198\n",
      " anybody: 0.530087947845459\n",
      " somebody: 0.5198413133621216\n",
      "\n",
      "[query] year\n",
      " month: 0.6911289691925049\n",
      " last: 0.6485914587974548\n",
      " quarter: 0.5974510908126831\n",
      " earlier: 0.5961364507675171\n",
      " june: 0.5774469375610352\n",
      "\n",
      "[query] car\n",
      " auto: 0.6898372769355774\n",
      " luxury: 0.5642927885055542\n",
      " cars: 0.5485533475875854\n",
      " corsica: 0.5310820937156677\n",
      " vehicle: 0.5268993377685547\n",
      "\n",
      "[query] toyota\n",
      " motor: 0.7580963373184204\n",
      " motors: 0.6888163089752197\n",
      " lexus: 0.6584320068359375\n",
      " nissan: 0.6549681425094604\n",
      " honda: 0.6451674699783325\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..\\..')\n",
    "import numpy as np\n",
    "from common.util import most_similar, create_co_matrix, ppmi\n",
    "\n",
    "\n",
    "window_sixe = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "print('counting co-occurrence ...')\n",
    "C = create_co_matrix(corpus, vocab_size, window_sixe)\n",
    "\n",
    "print('calsulateing PPMI ...')\n",
    "W = ppmi(C, verbose = True)\n",
    "\n",
    "print('calculateing SVD ...')\n",
    "try:\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5, random_state=None)\n",
    "except ImportError:\n",
    "    U, S, V = np.linalg.avd(W)\n",
    "    \n",
    "word_vecs = U[:, :wordvec_size]\n",
    "\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クエリyouに対してi, someone, we, anybody, somebodyなどの似たような働きを示す単語が上位に来ている。また、最も近いであろうiが類似度約0,6であり、上位2位以降が0.55程度であることを考えると、iが他の単語よりyouに類似していると言える。\\\n",
    "クエリyearに対してはmonth, last, quarter, earlier, juneなどの副詞に相当する単語が上位を占めている。中でも、年に意味的に近いmonthが類似度0.69程度であり、だいぶ類似していると言える。\\\n",
    "このように感覚的に近いと思える単語が上位に来ていることが確認できた。\\\n",
    "クエリcarの場合は同じ意味のautoが類似度約0.69で最も上位になっている。複数形carsよりも類似度が高い結果なっている。また、一見類似していないように思えるluxuryが上位に来ている。このように感覚から外れている単語が表れることもあることが分かった。\\\n",
    "クエリtoyotaの場合は、motor, motorsが上位に来ている。これはtoyotaとmotorの単語の結びつきが非常に強いことを示している。類似度も約0.76と非常に大きいことが分かる。また、上記3つの単語に比べ全体的に類似度が高いことも判断できる。motor(s)以外には競合の企業を表す単語が上位に来ている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] encourage\n",
      " educate: 0.5821194648742676\n",
      " attract: 0.47644418478012085\n",
      " convince: 0.46878278255462646\n",
      " persuade: 0.4561294615268707\n",
      " send: 0.4382459223270416\n",
      "\n",
      "[query] moment\n",
      " tremor: 0.47964078187942505\n",
      " hearing: 0.43014973402023315\n",
      " obstacle: 0.42400145530700684\n",
      " season: 0.4029232859611511\n",
      " tumultuous: 0.4003462493419647\n",
      "\n",
      "[query] hope\n",
      " chance: 0.43146514892578125\n",
      " ought: 0.4054609537124634\n",
      " doubt: 0.39464834332466125\n",
      " need: 0.38766375184059143\n",
      " trying: 0.37225764989852905\n",
      "\n",
      "[query] mix\n",
      " posture: 0.4752855896949768\n",
      " threats: 0.4746926724910736\n",
      " brains: 0.4531426429748535\n",
      " phones: 0.4395241141319275\n",
      " attitude: 0.4221952259540558\n",
      "\n",
      "[query] word\n",
      " rocks: 0.4503902792930603\n",
      " searches: 0.4176132380962372\n",
      " user: 0.4150398075580597\n",
      " handicapped: 0.4114261567592621\n",
      " time: 0.4011498689651489\n",
      "\n",
      "[query] spring\n",
      " winter: 0.5025620460510254\n",
      " june: 0.47368234395980835\n",
      " year: 0.4696305990219116\n",
      " march: 0.46523189544677734\n",
      " month: 0.4472695589065552\n"
     ]
    }
   ],
   "source": [
    "querys = ['encourage', 'moment', 'hope', 'mix', 'word', 'spring']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は理解を深めるために、類似度の高い組を見つけることを通して、類似度を高くする条件について考える。\\\n",
    "いくつか思いついた単語をクエリとして試した。全体的に類似度は0.5程度であり、参考文献1の実行例のように0.7程度になあるクエリではなかった。そこで一番類似度が高かったencourage, educateが動詞であることに注目して、一般的な動詞をクエリに選んで試した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] drive\n",
      " shake: 0.4771965444087982\n",
      " slow: 0.44867146015167236\n",
      " expansion: 0.4403308629989624\n",
      " lure: 0.4265044331550598\n",
      " ride: 0.41713976860046387\n",
      "\n",
      "[query] stand\n",
      " deck: 0.45150625705718994\n",
      " testify: 0.4342866837978363\n",
      " deliver: 0.42086249589920044\n",
      " shoot: 0.40984705090522766\n",
      " defend: 0.4088791012763977\n",
      "\n",
      "[query] move\n",
      " translate: 0.41032588481903076\n",
      " changes: 0.40304553508758545\n",
      " negotiate: 0.3943260908126831\n",
      " expansion: 0.39100009202957153\n",
      " complicate: 0.3790298104286194\n",
      "\n",
      "[query] touch\n",
      " audits: 0.44590747356414795\n",
      " finish: 0.4309748411178589\n",
      " ring: 0.42555928230285645\n",
      " chores: 0.41411811113357544\n",
      " plain: 0.40906715393066406\n",
      "\n",
      "[query] go\n",
      " come: 0.45910781621932983\n",
      " stay: 0.45697516202926636\n",
      " went: 0.4422570466995239\n",
      " take: 0.4241156578063965\n",
      " get: 0.4213069677352905\n",
      "\n",
      "[query] feel\n",
      " know: 0.598626971244812\n",
      " think: 0.5688796043395996\n",
      " guess: 0.5583916902542114\n",
      " bet: 0.5281607508659363\n",
      " sleep: 0.4856225848197937\n",
      "\n",
      "[query] say\n",
      " believe: 0.6351941823959351\n",
      " contend: 0.5858659148216248\n",
      " argue: 0.5484007596969604\n",
      " agree: 0.48842495679855347\n",
      " alike: 0.4717201590538025\n",
      "\n",
      "[query] come\n",
      " go: 0.45910781621932983\n",
      " reach: 0.4493756890296936\n",
      " get: 0.4373879134654999\n",
      " wait: 0.42031583189964294\n",
      " delivered: 0.41552823781967163\n"
     ]
    }
   ],
   "source": [
    "querys = ['drive', 'stand', 'move', 'touch', 'go', 'feel', 'say', 'come']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果は先ほどよりも類似度が大きい組を得ることができた。\\\n",
    "say, believeの組が0.64程度である。比較的高くなったsayやfeelは言い換えに相当する単語が存在していて、かつその単語も比較的頻繁に使われる単語であるからではないかと考えた。\\\n",
    "そこで「言う」として、talk, speak, tell, call, express, declare, state, assert, remarkを考えた。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] talk\n",
      " wipe: 0.40701234340667725\n",
      " dependents: 0.3980843126773834\n",
      " reports: 0.39481890201568604\n",
      " tell: 0.3836609125137329\n",
      " tables: 0.3788714110851288\n",
      "\n",
      "[query] speak\n",
      " lay: 0.5258762836456299\n",
      " check: 0.5136775970458984\n",
      " wipe: 0.5027220845222473\n",
      " induce: 0.5020589232444763\n",
      " catch: 0.4977559447288513\n",
      "\n",
      "[query] tell\n",
      " know: 0.7331713438034058\n",
      " appreciate: 0.6074075102806091\n",
      " understand: 0.6006683111190796\n",
      " forget: 0.6003093719482422\n",
      " wonder: 0.5781667232513428\n",
      "\n",
      "[query] call\n",
      " broker: 0.4073065519332886\n",
      " hear: 0.3777034282684326\n",
      " phone: 0.36717528104782104\n",
      " consider: 0.3663603663444519\n",
      " tell: 0.36570876836776733\n",
      "\n",
      "[query] express\n",
      " telephone: 0.5243455171585083\n",
      " ogilvy: 0.501406192779541\n",
      " railway: 0.4971642792224884\n",
      " american: 0.49487829208374023\n",
      " electric: 0.4837690591812134\n",
      "\n",
      "[query] declare\n",
      " exclusion: 0.4496135115623474\n",
      " hints: 0.43514642119407654\n",
      " repeated: 0.4001392722129822\n",
      " detected: 0.3786576986312866\n",
      " propose: 0.37473997473716736\n",
      "\n",
      "[query] state\n",
      " michigan: 0.5709823369979858\n",
      " federal: 0.5671976208686829\n",
      " appellate: 0.5388473272323608\n",
      " comptroller: 0.5212679505348206\n",
      " florida: 0.5004178881645203\n",
      "assert is not found\n",
      "\n",
      "[query] remark\n",
      " musical: 0.46277540922164917\n",
      " writes: 0.43544235825538635\n",
      " tells: 0.4331291913986206\n",
      " replies: 0.4286782741546631\n",
      " forget: 0.42552822828292847\n"
     ]
    }
   ],
   "source": [
    "querys = ['talk', 'speak', 'tell', 'call', 'express', 'declare', 'state', 'assert', 'remark']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果から判断すると、tellを除き、あまり類似度の高い組は得られていない。類似度の観点で単語の意味だけではなく他の要素も考慮しなくてはいけないことが分かった。\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] of\n",
      " the: 0.643458902835846\n",
      " in: 0.5346309542655945\n",
      " a: 0.524481475353241\n",
      " <unk>: 0.45984694361686707\n",
      " from: 0.42191675305366516\n",
      "\n",
      "[query] as\n",
      " such: 0.5284680128097534\n",
      " crystal: 0.37074601650238037\n",
      " well: 0.36144909262657166\n",
      " a: 0.34744393825531006\n",
      " succession: 0.33283576369285583\n",
      "\n",
      "[query] in\n",
      " of: 0.5346309542655945\n",
      " the: 0.5123343467712402\n",
      " and: 0.48073384165763855\n",
      " from: 0.4683004915714264\n",
      " <unk>: 0.45409372448921204\n",
      "\n",
      "[query] at\n",
      " economist: 0.47327396273612976\n",
      " strategist: 0.4428144693374634\n",
      " analyst: 0.4396021366119385\n",
      " dean: 0.42959871888160706\n",
      " par: 0.41116517782211304\n",
      "\n",
      "[query] on\n",
      " the: 0.4278296232223511\n",
      " of: 0.37433815002441406\n",
      " pbs: 0.35924166440963745\n",
      " jumbo: 0.3564925193786621\n",
      " c.d.s: 0.35245582461357117\n",
      "\n",
      "[query] from\n",
      " in: 0.4683004915714264\n",
      " <unk>: 0.446877121925354\n",
      " of: 0.42191675305366516\n",
      " and: 0.4070461690425873\n",
      " stemmed: 0.36175647377967834\n",
      "\n",
      "[query] by\n",
      " single-a-2: 0.43170565366744995\n",
      " single-a-1: 0.37230217456817627\n",
      " rated: 0.36099547147750854\n",
      " single-a-3: 0.36029380559921265\n",
      " triple-a: 0.354020893573761\n",
      "\n",
      "[query] with\n",
      " and: 0.35830652713775635\n",
      " equity-purchase: 0.3461391031742096\n",
      " <unk>: 0.33872178196907043\n",
      " between: 0.3387145698070526\n",
      " signed: 0.3340234160423279\n",
      "\n",
      "[query] along\n",
      " stands: 0.38736647367477417\n",
      " together: 0.3820379972457886\n",
      " ahead: 0.3697848618030548\n",
      " scene: 0.36464422941207886\n",
      " down: 0.36424049735069275\n",
      "\n",
      "[query] among\n",
      " dozens: 0.4069610834121704\n",
      " causing: 0.38162606954574585\n",
      " circus: 0.3756212592124939\n",
      " suppliers: 0.3538891077041626\n",
      " most: 0.31971943378448486\n"
     ]
    }
   ],
   "source": [
    "querys = ['of', 'as', 'in', 'at', 'on', 'from', 'by', 'with', 'along', 'among']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、クエリとして前置詞の場合を考えた。\\\n",
    "結果はsuch asやof theのように比較的でセットで使われることの多い単後の組が類似度が高かった。また、前置詞同士の組の類似度が高くなる傾向があることもわかった。\\\n",
    "一般的に多く使われる単語ではPMIの定義から真数部分の分母$C(x)C(y)$が大きくなってしまい、類似度があまり大きくならないということが確認できた。\\\n",
    "したがって類似度を高くするには定義から$C(x)C(y)$が小さく、$C(x,y)$が大きくなる、すなわち結びつきの強い単語が存在することが予想できるあまり一般的でない単語をクエリに選んであげると類似度の高い組が得られると考えられる。参考文献1の実行例だとhondaなどがよい例である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] 1980s\n",
      " holidays: 0.4526405334472656\n",
      " 1960s: 0.4193047881126404\n",
      " harvest: 0.3988034427165985\n",
      " televised: 0.38983404636383057\n",
      " found: 0.3893316090106964\n",
      "\n",
      "[query] california\n",
      " florida: 0.6350398063659668\n",
      " michigan: 0.6103793382644653\n",
      " oregon: 0.5962139368057251\n",
      " nevada: 0.592917799949646\n",
      " pennsylvania: 0.5928950309753418\n",
      "\n",
      "[query] nixon\n",
      " deng: 0.6064379811286926\n",
      " quayle: 0.5972130298614502\n",
      " 45-year-old: 0.5753530263900757\n",
      " blair: 0.5649703741073608\n",
      " reagan: 0.5600326657295227\n",
      "\n",
      "[query] hahn\n",
      " evans: 0.5528185367584229\n",
      " korotich: 0.5200653076171875\n",
      " welch: 0.4958854913711548\n",
      " savaiko: 0.493618905544281\n",
      " deaver: 0.4894590675830841\n",
      "\n",
      "[query] jerry\n",
      " gerald: 0.8000898957252502\n",
      " stephen: 0.7131688594818115\n",
      " patrick: 0.699432909488678\n",
      " joseph: 0.6870098114013672\n",
      " terry: 0.6804367303848267\n",
      "\n",
      "[query] fast-food\n",
      " chain: 0.6240389347076416\n",
      " store: 0.5996121168136597\n",
      " grocery: 0.5985656976699829\n",
      " chains: 0.5839478969573975\n",
      " restaurant: 0.5770061016082764\n",
      "\n",
      "[query] pasadena\n",
      " ana: 0.5689432621002197\n",
      " near: 0.5572552680969238\n",
      " cruz: 0.5480411648750305\n",
      " mateo: 0.5415921807289124\n",
      " creek: 0.5206318497657776\n",
      "\n",
      "[query] debentures\n",
      " subordinated: 0.9158099293708801\n",
      " eurobonds: 0.8386431336402893\n",
      " convertible: 0.8306756615638733\n",
      " notes: 0.8293894529342651\n",
      " due: 0.7728222608566284\n",
      "\n",
      "[query] aluminum\n",
      " minerals: 0.6257364749908447\n",
      " eli: 0.5642539262771606\n",
      " specialty: 0.546463131904602\n",
      " pharmaceuticals: 0.5415424704551697\n",
      " diesel: 0.5049399733543396\n"
     ]
    }
   ],
   "source": [
    "querys = ['1980s', 'california', 'nixon', 'hahn', 'jerry', 'fast-food', 'pasadena', 'debentures', 'aluminum']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ptb.train.txt内から上述したような特徴を持つと考えられる単語を適当に選び、実行した。\\\n",
    "結果は確かに類似度が高い組が得られた。特にjerryやdebenturesは上位の単語の類似度が非常に高く、上述した特徴を強く持っていると考えられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "類似度を高くするための条件について考えることを通して、よりカウントベースの手法についての理解が深まった。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
