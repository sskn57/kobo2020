{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第四回レポート課題\n",
    "参考文献1を参考に誤差逆伝搬法について学習した。\\\n",
    "学習にあたり、まず種々のレイヤの実装を実際に行い、理解を深めることとした"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.計算グラフについての理解\\\n",
    "ニューラルネットワークにおける処理の一つ一つをレイヤという単位で考えることができる。これにより開発が容易になる。\n",
    "逆伝搬により入力した値による出力値の偏微分の値を効率よく計算できる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.乗算レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#乗算レイヤ\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        #入力値の保存用\n",
    "        self.x = None \n",
    "        self.y = None\n",
    "    \n",
    "    #順伝播\n",
    "    def forward(self, x, y):\n",
    "        #入力値の保存\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        #乗算\n",
    "        out = x * y\n",
    "        return out\n",
    "    \n",
    "    #逆伝播\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100.0\n",
      "11.0 110.00000000000001 1000\n"
     ]
    }
   ],
   "source": [
    "#実行\n",
    "apple = 100\n",
    "apple_num = 10\n",
    "tax_rate = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "#forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax_rate)\n",
    "\n",
    "print(price)\n",
    "\n",
    "#backward\n",
    "dprice = 1\n",
    "dapple_price, dtax_rate = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確かに計算グラフ通りの値を出力している"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.加算レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        #加算\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5225.0\n",
      "11.0 110.00000000000001 27.500000000000004 165.0 4750\n"
     ]
    }
   ],
   "source": [
    "#実行\n",
    "apple = 100\n",
    "apple_num = 10\n",
    "orange = 150\n",
    "orange_num = 25\n",
    "tax_rate = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "#forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax_rate)\n",
    "\n",
    "print(price)\n",
    "\n",
    "#backward\n",
    "dprice = 1\n",
    "dall_price, dtax_rate = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dorange, dorange_num, dtax_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように出力値に対する各変数の偏微分した値をbackward()メソッドで求めることができる\n",
    "また、forwardとbackwardで処理する順番が真逆であることも確認できた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数の実装\n",
    "非線形な関数についても考える。また、スカラーではなくNumpy配列を扱うことを考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.RELUレイヤの実装\\\n",
    "$$y = x (x >  0)$$\n",
    "$$y = 0 (x <= 0)$$\n",
    "なので\n",
    "$$\\frac{\\partial y}{\\partial x} = 1 (x >  0)$$\n",
    "$$\\frac{\\partial y}{\\partial x} = 0 (x <= 0)$$\n",
    "になることに注意\\\n",
    "ReLUレイヤはスイッチのような役割を果たす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0) #0以下の場所をTrueとして記録した配列\n",
    "        out = x.copy() #複製(入力値への影響をなくす(そのまま代入だと参照渡しになるのでcopy()を用いる))\n",
    "        out[self.mask] = 0 #mask内でTrueの場所に0を代入\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0 #入力値が0以下は0に,0より大きい場合はそのまま\n",
    "        dx = dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[1. 0.]\n",
      " [0. 3.]]\n",
      "[[False  True]\n",
      " [ True False]]\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#実行\n",
    "import numpy as np\n",
    "x = np.array( [[ 1.0, -0.5], \n",
    "               [-2.0,  3.0]])\n",
    "\n",
    "print(x)\n",
    "\n",
    "relu_layer = Relu()\n",
    "\n",
    "#順伝播\n",
    "y = relu_layer.forward(x)\n",
    "print(y)\n",
    "\n",
    "print(relu_layer.mask)\n",
    "\n",
    "#逆伝播\n",
    "dout = np.array([[ 1.0, 1.0], \n",
    "               [1.0, 1.0]])\n",
    "dx = relu_layer.backward(dout)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Sigmoidレイヤの実装\\\n",
    "Sigmoid関数は次のように表すことができる\\\n",
    "$$y = \\frac{1}{1 + exp(-x)}$$\n",
    "また逆伝播の出力は以下のようになる。導出は参考文献1を参考にした。\\\n",
    "$$\\frac{\\partial L}{\\partial y} y^{2}exp(-x) = \\frac{\\partial L}{\\partial y} y (1 - y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        #出力結果(y)の保存用\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1.0 - self.out)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Affineレイヤの実装\\\n",
    "重みと入力値の線形和を取り、バイアスを加える処理をnumpy配列で行うことを考える\\\n",
    "逆伝播について、和に対してはスカラーの時同様にそのまま通せばよい。積のときは反対の入力値をかけて通すが行列の場合、行列のサイズに注意して転置して向きを考えてからかける。詳しくは参考文献1参照\\\n",
    " バイアスの加算については以下の性質(ブロードキャスト)に注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [11 12 13]]\n",
      "21\n",
      "[5 7 9]\n",
      "[ 6 15]\n"
     ]
    }
   ],
   "source": [
    "# 順伝播\n",
    "xw = np.array([[ 0,  0,  0], \n",
    "               [10, 10, 10]])\n",
    "b = np.array([1, 2, 3])\n",
    "y = xw + b #bが1×3から2×3にサイズが変わって加算されているこのとき列方向に拡張\n",
    "print(y)\n",
    "\n",
    "#逆伝播\n",
    "dy = np.array([[1, 2, 3], \n",
    "               [4, 5, 6]])\n",
    "db = np.sum(dy)\n",
    "print(db) \n",
    "db = np.sum(dy, axis = 0) #axis = 0 で列方向に処理\n",
    "print(db)\n",
    "db = np.sum(dy, axis = 1) #axis = 1 で行方向に処理\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W #weight\n",
    "        self.b = b #bias\n",
    "        self.x = None #入力保存用\n",
    "        self.dW = None #dW保存\n",
    "        self.db = None #db保存\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T) #∂L/∂x = ∂L/∂y(=dout)・W^t\n",
    "        self.dW = np.dot(self.x.T, dout) #∂L/∂w = x^t・∂L/∂y(=dout)\n",
    "        self.db = np.sum(dout, axis = 0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (5, 2)\n",
      "[[0.2353646  0.07768556]\n",
      " [0.10410011 0.47737009]\n",
      " [0.4696064  0.20996369]\n",
      " [0.31006524 0.01962232]\n",
      " [0.95613636 0.1596323 ]]\n",
      "w (2, 3)\n",
      "[[0.1804642  0.82047527 0.60915726]\n",
      " [0.92056947 0.95612174 0.08416438]]\n",
      "b (1, 3)\n",
      "[[0.6979588  0.7935734  0.28061881]]\n",
      "y (5, 3)\n",
      "[[0.81194864 1.06096108 0.43053122]\n",
      " [1.15619747 1.33540888 0.3842097 ]\n",
      " [0.97599211 1.37962469 0.58435442]\n",
      " [0.77197818 1.06673559 0.4711488 ]\n",
      " [1.01745981 1.73068755 0.87649156]]\n",
      "dx (5, 2)\n",
      "[[1.2792812  1.79809848]\n",
      " [1.53836634 2.37351033]\n",
      " [1.6640433  2.26673952]\n",
      " [1.3015483  1.77024258]\n",
      " [2.13752259 2.66515979]]\n",
      "dW (2, 3)\n",
      "[[1.9819903  3.02214038 1.39987696]\n",
      " [0.99750112 1.30678216 0.48871088]]\n",
      "db (3,)\n",
      "[4.7335762  6.57341778 2.74673569]\n"
     ]
    }
   ],
   "source": [
    "#実行\n",
    "x = np.random.rand(5, 2) #5*2\n",
    "w = np.random.rand(2, 3) #2*3\n",
    "b = np.random.rand(1, 3) #1*3\n",
    "#y (2*4)*(4*3) = 5*3\n",
    "print('x {}'.format(x.shape))\n",
    "print(x)\n",
    "print('w {}'.format(w.shape))\n",
    "print(w)\n",
    "print('b {}'.format(b.shape))\n",
    "print(b)\n",
    "\n",
    "#順伝播\n",
    "affine_layer = Affine(w, b)\n",
    "y = affine_layer.forward(x)\n",
    "print('y {}'.format(y.shape))\n",
    "print(y) #5*3\n",
    "\n",
    "#逆伝播\n",
    "dx = affine_layer.backward(y)\n",
    "print('dx {}'.format(dx.shape))\n",
    "print(dx)\n",
    "print('dW {}'.format(affine_layer.dW.shape))\n",
    "print(affine_layer.dW)\n",
    "print('db {}'.format(affine_layer.db.shape))\n",
    "print(affine_layer.db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文献1の図5-27に相当する。$N = 5$として考えた場合である。\\\n",
    "サイズに注目すると、慣れないベクトル、行列の偏微分なども分かりやすくなった。\\\n",
    "Affineはニューラルネットワークにおいて活性化関数と交互に出てくる重要なレイヤなのでしっかり理解できてよかった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Softmax-with-Lossレイヤの実装\\\n",
    "正規化して出力するソフトマックス関数と、損失関数交差エントロピー誤差を合わせたレイヤを考える。\\\n",
    "順伝播や逆伝播の導出は煩雑なので結果だけに注目することとした。\\\n",
    "$(a1, a2, a3)$ Softmaxへの入力\\\n",
    "$(y1, y2, y3)$ Softmaxからの出力 & Cross Emtropy Errorへの入力\\\n",
    "$(t1, t2, t3)$ Cross Emtropy Errorへの入力(教師データ)\\\n",
    "とすると、各入力への逆伝播は\\\n",
    "$(y1 - t1, y2 - t2, y3 - t3)$\n",
    "になる。\\\n",
    "この逆伝播の値はニューラルネットワークと教師データの差であり、精度のようなものであるといえる。したがって逆伝播の値を小さくするように考えていけばよい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss():\n",
    "    def __init__(self):\n",
    "        self.loss = None #損失関数保存\n",
    "        self.y = None #Softmax出力保存\n",
    "        self.t = None #教師データ保存\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout  = 1):\n",
    "        batch_size = self.t.shape[0] #入力データ数\n",
    "        dx = (self.y - self.t) / batch_size #正規化\n",
    "        return dx\n",
    "\n",
    "def softmax(a):\n",
    "    c = np.max(a) #最大値計算\n",
    "    exp_a = np.exp(a - c) #入力を定数ずらすのは問題ない、最大値分下げることでオーバーフロー対策\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log( y[np.arange(batch_size), t] + 1e-7 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t (3,)\n",
      "[0 1 0]\n",
      "x (3,)\n",
      "[0.22944171 0.72390616 0.37398078]\n",
      "loss\n",
      "3.5066879851515314\n",
      "dx (3,)\n",
      "[ 0.08783194 -0.18932234  0.1014904 ]\n",
      "abs_dx\n",
      "[0.08783194 0.18932234 0.1014904 ]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#実行\n",
    "t = np.array([0, 1, 0])\n",
    "x = np.random.rand(3)\n",
    "print('t {}'.format(t.shape))\n",
    "print(t)\n",
    "print('x {}'.format(x.shape))\n",
    "print(x)\n",
    "\n",
    "#順伝播\n",
    "softmaxwithloss_layer = SoftmaxWithLoss()\n",
    "l = softmaxwithloss_layer.forward(x, t)\n",
    "print('loss')\n",
    "print(l)\n",
    "\n",
    "#逆伝播\n",
    "dx = softmaxwithloss_layer.backward()\n",
    "print('dx {}'.format(dx.shape))\n",
    "print(dx)\n",
    "\n",
    "abs_dx = np.abs(dx)\n",
    "print('abs_dx')\n",
    "print(abs_dx)\n",
    "print(np.argmin(abs_dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果を見ると、たしかに逆伝播を用いて精度の評価ができている。\\\n",
    "教師データを[0, 1, 0]として実行すると、損失関数l及び、その偏微分dxが計算できている。\\\n",
    "また、入力の中で教師データと一番近いものをdxの絶対値の大きさを用いて判断出来ている。\\\n",
    "したがってdxをもちいて前回学んだ勾配降下法を行うことができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでで各レイヤの理解が深まったので、ここからはサンプルコードを参考に、誤差逆伝播法に対応したニューラルネットワークを実装する。\n",
    "前回同様、二層で考える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        #weight,biasの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        #layer\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()   \n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values(): #0層から順にforward処理(lastlayerは行わない)\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t) #損失関数の計算\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1) #行ごとに処理、最大値のインデックスを求める\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis = 1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        #forward\n",
    "        self.loss(x, t) #損失関数の計算\n",
    "        \n",
    "        #backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout) #\n",
    "        \n",
    "        layers = list(self.layers.values()) #forwardの順にレイヤが格納されたリスト\n",
    "        layers.reverse() #backwardの順に変更\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout) #各層のbackward処理を行う,最後まで行くと入力値まで求まる\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:2.580907633960024e-13\n",
      "b1:8.865927591150853e-13\n",
      "W2:9.42440426426102e-13\n",
      "b2:1.205702274131859e-10\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "#実行\n",
    "#データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "# 数値微分\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "# Backward\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "#数値微分と誤差逆伝播法により計算された勾配の差の絶対値を計算\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TwoLayerNetクラス内に誤差逆伝播法による勾配計算するメソッドを実装した。\\\n",
    "精度については勾配の差が1e-10以上の精度で求まっており、前回の結果と合わせると二つとも十分であるといえる。\\\n",
    "実行速度について考察する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00099945068359375 7.4606852531433105 -7.459685802459717\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "numerical_start = time.time()\n",
    "network.numerical_gradient(x_batch, t_batch)\n",
    "numerical_time = time.time() - numerical_start\n",
    "\n",
    "start = time.time()\n",
    "network.gradient(x_batch, t_batch)\n",
    "time = time.time() - start\n",
    "\n",
    "print (time, numerical_time, time - numerical_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文献6を参考に勾配計算にかかる時間を計測した。timeは誤差逆伝搬法, numerical_timeは数値微分である。\\\n",
    "timeは0.0009となっていて、処理にほとんど時間がかかっていないが、numerical_timeは7.4程度であり、数値微分よりも誤差逆伝搬法のほうが実行速度が速いことが確認できた。今回のデータ数で、\n",
    "$$\\frac{7.4}{0.0009} \\simeq 8200$$\n",
    "となり、およそ8200倍である。データ数が増えればさらにこの差は大きくなると考えられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、前回の内容と合わせたまとめとして、誤差逆伝搬法を用いた学習について考える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18816666666666668, 0.1865, rate = 0.0016666666666666774\n",
      "0.90495, 0.9099, rate = -0.00495000000000001\n",
      "0.9178333333333333, 0.9198, rate = -0.001966666666666672\n",
      "0.9332333333333334, 0.9335, rate = -0.0002666666666666373\n",
      "0.9410833333333334, 0.9397, rate = 0.001383333333333403\n",
      "0.9510166666666666, 0.9482, rate = 0.0028166666666665785\n",
      "0.95585, 0.953, rate = 0.002850000000000019\n",
      "0.9592333333333334, 0.9564, rate = 0.0028333333333333544\n",
      "0.9634333333333334, 0.9587, rate = 0.004733333333333367\n",
      "0.9674166666666667, 0.9633, rate = 0.004116666666666657\n",
      "0.9688666666666667, 0.9636, rate = 0.005266666666666642\n",
      "0.9719, 0.9646, rate = 0.007299999999999973\n",
      "0.9737166666666667, 0.9667, rate = 0.007016666666666671\n",
      "0.9736, 0.9664, rate = 0.007199999999999984\n",
      "0.9764666666666667, 0.9671, rate = 0.009366666666666745\n",
      "0.9758666666666667, 0.9676, rate = 0.008266666666666644\n",
      "0.9791, 0.9687, rate = 0.010399999999999965\n",
      "last_rate = 0.010621999795730738\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "#NNの作成\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "#ハイパーパラメータ\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "#各種パラメータ推移記録用リスト\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 誤差逆伝搬法による勾配計算\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    #accuracy保存\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print('{}, {}, rate = {}'.format(train_acc, test_acc, train_acc - test_acc) )\n",
    "        \n",
    "last_acc_dif = abs( train_acc_list[-1] - test_acc_list[-1] )\n",
    "rate = last_acc_dif / train_acc_list[-1]\n",
    "print('last_rate = {}'.format(rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train, testいずれも最終的な精度は95%を超えていて, 精度の観点では問題はない。また、二つのaccuracyの差は最終的に約0.01であり、約1%認められた。したがってわずかに過学習を起こしているが、問題ない範囲だと考えた。前回の数値微分法と比べても精度の点ではあまり変化はないようである。実行速度の点で優れている。実行速度は大量のデータを扱うこの分野においては非常に重要な要素であるため、多少実装が複雑でも誤差逆伝搬法を用いるべきだといえる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文献\n",
    "1. ゼロから作る Deep Learning Pythonで学ぶディープラーニングの理論と実装\n",
    "2. https://qiita.com/mytk0u0/items/231807f4136b2b1681b0 (copyメソッドについて調べるのに利用)\n",
    "3. https://tutorials.chainer.org/ja/05_Basics_of_Linear_Algebra.html (行列の偏微分について調べるのに利用)\n",
    "4. https://note.nkmk.me/python-numpy-transpose/ (numpy配列の転置について調べるのに利用)\n",
    "5. https://www.sejuku.net/blog/70994 (numpyでの絶対値の使い方を調べるのに利用)\n",
    "6. https://qiita.com/fantm21/items/3dc7fbf4e935311488bc (Python処理にかかる時間について調べるのに利用)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
